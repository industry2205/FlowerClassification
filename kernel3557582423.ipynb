{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import random, re, math\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nimport tensorflow as tf, tensorflow.keras.backend as K\nfrom kaggle_datasets import KaggleDatasets\nprint('Tensorflow version ' + tf.__version__)\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configurations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nIMAGE_SIZE = [512, 512]\nEPOCHS = 25\nFOLDS = 2\n# SEED = 777\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MIXED_PRECISION = False\nXLA_ACCELERATE = False\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Directories"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('flower-classification-with-tpus')\n\nGCS_PATH_SELECT = { # available image sizes\n    192: GCS_DS_PATH + '/tfrecords-jpeg-192x192',\n    224: GCS_DS_PATH + '/tfrecords-jpeg-224x224',\n    331: GCS_DS_PATH + '/tfrecords-jpeg-331x331',\n    512: GCS_DS_PATH + '/tfrecords-jpeg-512x512'\n}\n\nGCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('tf-flower-photo-tfrec')\n# GCS_DS_PATH = glob.glob(GCS_DS_PATH + '/*')\n\nGCS_PATH_SELECT = { # available image sizes\n    512: GCS_DS_PATH + '/*/tfrecords-jpeg-512x512'\n}\n\nGCS_PATH1 = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec') + tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec') + tf.io.gfile.glob(GCS_PATH1 + '/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec') # predictions on this dataset should be submitted for the competition","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"CLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 102","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom LR scheduler\nFrom starter [kernel][1]\n\n[1]: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Learning rate schedule for TPU, GPU and CPU.\n# Using an LR ramp up because fine-tuning a pre-trained model.\n# Starting with a high LR would break the pre-trained weights.\n\nLR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n\nrng = [i for i in range(25 if EPOCHS<25 else EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset Functions\nFrom starter [kernel][1]\n\n[1]: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3])\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3])    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3])\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3])\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n\ndef transform(image,label):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SIZE[0]\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM,DIM,3]),label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled = True, ordered = False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n    \n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n        \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # use data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n    return dataset\n\ndef data_augment(image, label):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    return image, label   \n\ndef get_training_dataset(dataset,do_aug=True):\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    if do_aug: dataset = dataset.map(transform, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(dataset):\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nNUM_TRAINING_IMAGES = int( count_data_items(TRAINING_FILENAMES))\n# NUM_VALIDATION_IMAGES = int( count_data_items(TRAINING_FILENAMES) )\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n\nprint('Dataset: {} training images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES,NUM_TEST_IMAGES))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = load_dataset(TRAINING_FILENAMES, labeled=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def decode_image(image_data):\n#     image = tf.image.decode_jpeg(image_data, channels=3)\n#     image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n#     image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n#     return image\n\n# def read_labeled_tfrecord(example):\n#     LABELED_TFREC_FORMAT = {\n#         \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n#         \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n#     }\n#     example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n#     image = decode_image(example['image'])\n#     label = tf.cast(example['class'], tf.int32)\n#     return image, label # returns a dataset of (image, label) pairs\n\n# def read_unlabeled_tfrecord(example):\n#     UNLABELED_TFREC_FORMAT = {\n#         \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n#         \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n#         # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n#     }\n#     example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n#     image = decode_image(example['image'])\n#     idnum = example['id']\n#     return image, idnum # returns a dataset of image(s)\n\n# def load_dataset(filenames, labeled = True, ordered = False):\n#     # Read from TFRecords. For optimal performance, reading from multiple files at once and\n#     # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n    \n#     ignore_order = tf.data.Options()\n#     if not ordered:\n#         ignore_order.experimental_deterministic = False # disable order, increase speed\n        \n#     dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO) # automatically interleaves reads from multiple files\n#     dataset = dataset.with_options(ignore_order) # use data as soon as it streams in, rather than in its original order\n#     dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n#     return dataset\n\n# def data_augment(image, label):\n#     # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n#     # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n#     # of the TPU while the TPU itself is computing gradients.\n#     image = tf.image.random_flip_left_right(image)\n#     return image, label   \n\n# def get_training_dataset(dataset,do_aug=True):\n#     dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n#     dataset = dataset.map(transform, num_parallel_calls=AUTO)\n#     dataset = dataset.repeat() # the training dataset must repeat for several epochs\n#     dataset = dataset.shuffle(2048)\n#     dataset = dataset.batch(BATCH_SIZE)\n#     dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n#     return dataset\n\n# def get_validation_dataset(dataset):\n#     dataset = dataset.batch(BATCH_SIZE)\n#     dataset = dataset.cache()\n#     dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n#     return dataset\n\n# def get_test_dataset(ordered=False):\n#     dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n#     dataset = dataset.batch(BATCH_SIZE)\n#     dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n#     return dataset\n\n# def count_data_items(filenames):\n#     # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n#     n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n#     return np.sum(n)\n\n# NUM_TRAINING_IMAGES = int( count_data_items(TRAINING_FILENAMES))\n# # NUM_VALIDATION_IMAGES = int( count_data_items(TRAINING_FILENAMES) * (1./FOLDS) )\n# NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n# STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n\n# print('Dataset: {} training images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_TEST_IMAGES))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Display Example Augmentation\nBelow are examples of 3 training images where each is randomly augmented 12 different times."},{"metadata":{"trusted":true},"cell_type":"code","source":"# row = 3; col = 4;\n# all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\n# one_element = tf.data.Dataset.from_tensors(next(iter(all_elements)))\n# augmented_element = one_element.repeat().map(transform).batch(row*col)\n\n# for (img,label) in augmented_element:\n#     plt.figure(figsize=(15,int(15*row/col)))\n#     for j in range(row*col):\n#         plt.subplot(row,col,j+1)\n#         plt.axis('off')\n#         plt.imshow(img[j,])\n#     plt.show()\n#     break","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# row = 3; col = 4;\n# all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\n# one_element = tf.data.Dataset.from_tensors(next(iter(all_elements)))\n# augmented_element = one_element.repeat().map(transform).batch(row*col)\n\n# for (img,label) in augmented_element:\n#     plt.figure(figsize=(15,int(15*row/col)))\n#     for j in range(row*col):\n#         plt.subplot(row,col,j+1)\n#         plt.axis('off')\n#         plt.imshow(img[j,])\n#     plt.show()\n#     break","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# row = 3; col = 4;\n# all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\n# one_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\n# augmented_element = one_element.repeat().map(transform).batch(row*col)\n\n# for (img,label) in augmented_element:\n#     plt.figure(figsize=(15,int(15*row/col)))\n#     for j in range(row*col):\n#         plt.subplot(row,col,j+1)\n#         plt.axis('off')\n#         plt.imshow(img[j,])\n#     plt.show()\n#     break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build, Train, Infer Model\nThis is the 5-Fold workflow copied from Ragnar's notebook [here][1]. Now we add data augmentation to the training images on the fly! Notice how his notebook completes epochs in 70 seconds using TPU. This notebook also completes epochs in 70 seconds (when we turn on TPU) and we are augmentating every image! Augmenting a single image requires 5,000,000 calculations (a batch requires 600,000,000 calculations!) We see that our augmentation is occuring as fast as the GPU/TPU training! We are augmenting 200+ images per second. In other words we are performing 1,000,000,000 calculations per second in addition to normal training computation! Wow!\n\n[1]: https://www.kaggle.com/ragnar123/5-kfold-densenet201"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q efficientnet\nimport efficientnet.tfkeras as efn\nfrom tensorflow.keras.applications import DenseNet201","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # def get_model():\n# #     with strategy.scope():\n# #         rnet = efn.EfficientNetB7(\n# #             input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n# #             weights='imagenet',\n# #             include_top=False\n# #         )\n# #         # trainable rnet\n# #         rnet.trainable = True\n# #         model = tf.keras.Sequential([\n# #             rnet,\n# #             tf.keras.layers.GlobalAveragePooling2D(),\n# #             tf.keras.layers.Dense(len(CLASSES), activation='softmax',dtype='float32')\n# #         ])\n# #     model.compile(\n# #         optimizer='adam',\n# #         loss = 'sparse_categorical_crossentropy',\n# #         metrics=['sparse_categorical_accuracy']\n# #     )\n# #     return model\n\n\n# def get_model():\n#     with strategy.scope():\n#         rnet = DenseNet201(\n#             input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n#             weights='imagenet',\n#             include_top=False\n#         )\n#         # trainable rnet\n#         rnet.trainable = True\n#         model = tf.keras.Sequential([\n#             rnet,\n#             tf.keras.layers.GlobalAveragePooling2D(),\n#             tf.keras.layers.Dense(len(CLASSES), activation='softmax',dtype='float32')\n#         ])\n#     model.compile(\n#         optimizer='adam',\n#         loss = 'sparse_categorical_crossentropy',\n#         metrics=['sparse_categorical_accuracy']\n#     )\n#     return model\n\n\n# model = get_model()\n\n# def train_cross_validate(folds = 2):\n#     histories = []\n#     models = []\n#     early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3)\n#     kfold = KFold(folds, shuffle = True, random_state = SEED)\n#     for f, (trn_ind, val_ind) in enumerate(kfold.split(TRAINING_FILENAMES)):\n#         print(); print('#'*25)\n#         print('### FOLD',f+1)\n#         print('#'*25)\n#         train_dataset = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[trn_ind]['TRAINING_FILENAMES']), labeled = True)\n# #         val_dataset = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[val_ind]['TRAINING_FILENAMES']), labeled = True, ordered = True)\n#         model = get_model()\n#         history = model.fit(\n#             get_training_dataset(train_dataset),\n#             steps_per_epoch = STEPS_PER_EPOCH,\n#             epochs = EPOCHS,\n#             callbacks = [lr_callback, early_stopping],\n#         )\n#         models.append(model)\n#         histories.append(history)\n#     return histories, models\n\n# def train_and_predict(folds = 2):\n#     test_ds = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n#     test_images_ds = test_ds.map(lambda image, idnum: image)\n#     print('Start training %i folds'%folds)\n#     histories, models = train_cross_validate(folds = folds)\n#     print('Computing predictions...')\n#     # get the mean probability of the folds models\n#     probabilities = np.average([models[i].predict(test_images_ds) for i in range(folds)], axis = 0)\n#     predictions = np.argmax(probabilities, axis=-1)\n#     print('Generating submission.csv file...')\n#     test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n#     test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n#     np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n#     return histories, models\n    \n# # run train and predict\n# histories, models = train_and_predict(folds = FOLDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from tensorflow.keras.applications import DenseNet201\n# import efficientnet.tfkeras as efn\n\n# def get_model():\n#     with strategy.scope():\n#         rnet = efn.EfficientNetB7(\n#             input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n#             weights='noisy-student',\n#             include_top=False\n#         )\n#         # trainable rnet\n#         rnet.trainable = True\n#         model = tf.keras.Sequential([\n#             rnet,\n#             tf.keras.layers.GlobalAveragePooling2D(),\n#             tf.keras.layers.Dense(len(CLASSES), activation='softmax',dtype='float32')\n#         ])\n#     model.compile(\n#         optimizer='adam',\n#         loss = 'sparse_categorical_crossentropy',\n#         metrics=['sparse_categorical_accuracy']\n#     )\n#     return model\n\n# def train_cross_validate(folds = 5):\n#     histories = []\n#     models = []\n#     early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3)\n#     kfold = KFold(folds, shuffle = True, random_state = SEED)\n#     for f, (trn_ind, val_ind) in enumerate(kfold.split(TRAINING_FILENAMES)):\n#         print(); print('#'*25)\n#         print('### FOLD',f+1)\n#         print('#'*25)\n#         train_dataset = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[trn_ind]['TRAINING_FILENAMES']), labeled = True)\n#         val_dataset = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[val_ind]['TRAINING_FILENAMES']), labeled = True, ordered = True)\n#         model = get_model()\n#         history = model.fit(\n#             get_training_dataset(train_dataset), \n#             steps_per_epoch = STEPS_PER_EPOCH,\n#             epochs = EPOCHS,\n#             callbacks = [lr_callback],#, early_stopping],\n#             validation_data = get_validation_dataset(val_dataset),\n#             verbose=2\n#         )\n#         models.append(model)\n#         histories.append(history)\n#     return histories, models\n\n# def train_and_predict(folds = 5):\n#     test_ds = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n#     test_images_ds = test_ds.map(lambda image, idnum: image)\n#     print('Start training %i folds'%folds)\n#     histories, models = train_cross_validate(folds = folds)\n#     print('Computing predictions...')\n#     # get the mean probability of the folds models\n#     probabilities = np.average([models[i].predict(test_images_ds) for i in range(folds)], axis = 0)\n#     predictions = np.argmax(probabilities, axis=-1)\n#     print('Generating submission.csv file...')\n#     test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n#     test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n#     np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n#     return histories, models\n    \n# # run train and predict\n# histories, models = train_and_predict(folds = FOLDS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix and Validation Score\nTry forking and modifying this notebook to maximize validation score below. Tune the data augmentation and/or train for more epochs to increase accuracy. Good luck! (Code below is from starter [kernel][1]).\n\n[1]: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def display_confusion_matrix(cmat, score, precision, recall):\n#     plt.figure(figsize=(15,15))\n#     ax = plt.gca()\n#     ax.matshow(cmat, cmap='Reds')\n#     ax.set_xticks(range(len(CLASSES)))\n#     ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})\n#     plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n#     ax.set_yticks(range(len(CLASSES)))\n#     ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})\n#     plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n#     titlestring = \"\"\n#     if score is not None:\n#         titlestring += 'f1 = {:.3f} '.format(score)\n#     if precision is not None:\n#         titlestring += '\\nprecision = {:.3f} '.format(precision)\n#     if recall is not None:\n#         titlestring += '\\nrecall = {:.3f} '.format(recall)\n#     if len(titlestring) > 0:\n#         ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# %%time\n# all_labels = []; all_prob = []; all_pred = []\n# kfold = KFold(FOLDS, shuffle = True, random_state = SEED)\n# for j, (trn_ind, val_ind) in enumerate( kfold.split(TRAINING_FILENAMES) ):\n#     print('Inferring fold',j+1,'validation images...')\n#     VAL_FILES = list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[val_ind]['TRAINING_FILENAMES'])\n#     NUM_VALIDATION_IMAGES = count_data_items(VAL_FILES)\n#     cmdataset = get_validation_dataset(load_dataset(VAL_FILES, labeled = True, ordered = True))\n#     images_ds = cmdataset.map(lambda image, label: image)\n#     labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n#     all_labels.append( next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy() ) # get everything as one batch\n#     prob = models[j].predict(images_ds)\n#     all_prob.append( prob )\n#     all_pred.append( np.argmax(prob, axis=-1) )\n# cm_correct_labels = np.concatenate(all_labels)\n# cm_probabilities = np.concatenate(all_prob)\n# cm_predictions = np.concatenate(all_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(\"Correct   labels: \", cm_correct_labels.shape, cm_correct_labels)\n# print(\"Predicted labels: \", cm_predictions.shape, cm_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)))\n# score = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n# precision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n# recall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n# display_confusion_matrix(cmat, score, precision, recall)\n# print('f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(score, precision, recall))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Efficientnet + Densenet"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_START = 0.0001\nLR_MAX = 0.000000015 * strategy.num_replicas_in_sync\nLR_MIN = 0.00000005\nLR_RAMPUP_EPOCHS = 4\nLR_SUSTAIN_EPOCHS = 6\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n#     if epoch < LR_RAMPUP_EPOCHS:\n#         lr = np.random.random_sample() * LR_START\n#     elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n#         lr = LR_MAX\n#     else:\n    lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch) + LR_MIN\n#     lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nes_callback = tf.keras.callbacks.EarlyStopping(min_delta=0, patience=10, verbose=1, mode='auto', restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with strategy.scope():\n#     dn201 = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n#     dn201.trainable = True # Full Training\n    \n#     model1 = tf.keras.Sequential([\n#         dn201,\n#         tf.keras.layers.GlobalAveragePooling2D(),\n#         tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n#     ])\n        \n# model1.compile(\n#     optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n#     loss = 'sparse_categorical_crossentropy',\n#     metrics=['sparse_categorical_accuracy']\n# )\n# model1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with strategy.scope():\n#     enb7 = efn.EfficientNetB7(weights='noisy-student', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n#     enb7.trainable = True # Full Training\n    \n#     model2 = tf.keras.Sequential([\n#         enb7,\n#         tf.keras.layers.GlobalAveragePooling2D(),\n#         tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n#     ])\n\n# model2.load_weights('/kaggle/input/flowermodel/flower_efficientnet_final1.h5')\n    \n# model2.compile(\n#     optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n#     loss = 'sparse_categorical_crossentropy',\n#     metrics=['sparse_categorical_accuracy']\n# )\n# model2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# history1 = model1.fit(get_training_dataset(dataset), steps_per_epoch=STEPS_PER_EPOCH, epochs=30, callbacks = [lr_callback,es_callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model1.save('flower_densenet.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# history2 = model2.fit(get_training_dataset(dataset), steps_per_epoch=STEPS_PER_EPOCH, epochs=6, callbacks = [lr_callback,es_callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model2.save('flower_efficientnet_final1.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = tf.keras.models.load_model('/kaggle/input/flowermodel/flower_densenet_final1.h5')\nmodel2 = tf.keras.models.load_model('/kaggle/input/flowermodel/flower_efficientnet_final1.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_alpha = 0.7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ds = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n\nprint('Computing predictions...')\ntest_images_ds = test_ds.map(lambda image, idnum: image)\n#probabilities = (model1.predict(test_images_ds)+model2.predict(test_images_ds))/2 # using average model probability\nprobabilities = model1.predict(test_images_ds)*(1-best_alpha) + model2.predict(test_images_ds)*best_alpha\n# probabilities = model2.predict(test_images_ds)\npredictions = np.argmax(probabilities, axis=-1)\nprint(predictions)\n\nprint('Generating submission.csv file...')\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\nnp.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n!head submission.csv","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}